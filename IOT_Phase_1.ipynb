{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHeVvTHZ-IcC",
        "outputId": "5610ac72-37ef-4886-ecbf-49940bd9aaf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Columns: Index(['time', 'ax', 'ay', 'az', 'wx', 'wy', 'wz', 'Bx', 'By', 'Bz'], dtype='object')\n",
            "Cleaned Columns: Index(['time', 'ax', 'ay', 'az', 'wx', 'wy', 'wz', 'Bx', 'By', 'Bz'], dtype='object')\n",
            "Data types:\n",
            " time    float64\n",
            "ax      float64\n",
            "ay      float64\n",
            "az      float64\n",
            "wx      float64\n",
            "wy      float64\n",
            "wz      float64\n",
            "Bx      float64\n",
            "By      float64\n",
            "Bz      float64\n",
            "dtype: object\n",
            "Missing values:\n",
            " time    0\n",
            "ax      0\n",
            "ay      0\n",
            "az      0\n",
            "wx      0\n",
            "wy      0\n",
            "wz      0\n",
            "Bx      0\n",
            "By      0\n",
            "Bz      0\n",
            "dtype: int64\n",
            "Detected Peaks at Indices: [ 29  47  55  72  85  98 116 124 132 149 159 179 192 201 244 253 274 287\n",
            " 300 309 317 335 343 356 382 391 403 411 420 434 442 464 477 493 502 520\n",
            " 532 550 558 567]\n",
            "Preprocessed data saved to: processed_fall_data.xlsx\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "\n",
        "# Section 1: Load Data\n",
        "file_path = 'Forward_1.xlsx'  # Ensure the file path is correct\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Print the column names for debugging\n",
        "print(\"Original Columns:\", df.columns)\n",
        "\n",
        "# Clean column names (remove leading/trailing whitespace)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Verify column names again\n",
        "print(\"Cleaned Columns:\", df.columns)\n",
        "\n",
        "# Section 2: Preprocess Data\n",
        "# Ensure numeric data types for relevant columns\n",
        "for col in ['ax', 'ay', 'az', 'wx', 'wy', 'wz', 'Bx', 'By', 'Bz']:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric, setting errors to NaN\n",
        "\n",
        "# Check for missing values after conversion\n",
        "print(\"Data types:\\n\", df.dtypes)\n",
        "print(\"Missing values:\\n\", df.isnull().sum())\n",
        "\n",
        "# Calculate total acceleration\n",
        "df['total_accel'] = np.sqrt(df['ax']**2 + df['ay']**2 + df['az']**2)\n",
        "\n",
        "# Section 3: Detect Peaks\n",
        "# Detect peaks in the total acceleration\n",
        "peaks, _ = signal.find_peaks(df['total_accel'], height=1.0)  # Adjust height threshold as needed\n",
        "print(\"Detected Peaks at Indices:\", peaks)\n",
        "\n",
        "# Initialize lists to store fall direction and event information\n",
        "fall_direction = []\n",
        "fall_event = []\n",
        "\n",
        "# Section 4: Extract Features around Peaks\n",
        "for peak in peaks:\n",
        "    if peak > 50 and peak < len(df) - 50:  # Ensure there's data around the peak\n",
        "        segment = df.iloc[peak-50:peak+50]  # Get 50 samples before and after\n",
        "        fall_direction.append('forward')  # Assuming direction is known\n",
        "        fall_event.append(1)  # Mark as fall event\n",
        "    else:\n",
        "        fall_direction.append(np.nan)\n",
        "        fall_event.append(0)\n",
        "\n",
        "# Extend lists to match the DataFrame length\n",
        "fall_direction = fall_direction + [np.nan] * (len(df) - len(fall_direction))\n",
        "fall_event = fall_event + [0] * (len(df) - len(fall_event))\n",
        "\n",
        "# Add new columns to the DataFrame\n",
        "df['Fall'] = fall_event\n",
        "df['Direction'] = fall_direction\n",
        "\n",
        "# Section 5: Feature Extraction\n",
        "def extract_features(segment):\n",
        "    # Check if segment['total_accel'] is NaN\n",
        "    if pd.isna(segment['total_accel']):  # Check for single NaN value\n",
        "        print(\"Segment contains null values, skipping.\")\n",
        "        return pd.Series()  # Return an empty series if any null values are present\n",
        "\n",
        "    # Since segment is a single row, we can create a window around it for the calculations\n",
        "    features = {\n",
        "        'min': segment['total_accel'],\n",
        "        'max': segment['total_accel'],\n",
        "        'mean': segment['total_accel'],\n",
        "        'skewness': np.nan,  # You may want to calculate skewness if you have a window of data\n",
        "        'kurtosis': np.nan,  # Same for kurtosis\n",
        "        'autocorrelation': np.nan,  # Autocorrelation also needs more data\n",
        "    }\n",
        "    return pd.Series(features)\n",
        "\n",
        "\n",
        "# Create a feature DataFrame\n",
        "features_df = df.iloc[peaks].apply(extract_features, axis=1)\n",
        "\n",
        "# Section 6: Save Preprocessed Data\n",
        "output_file_path = 'processed_fall_data.xlsx'\n",
        "df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(\"Preprocessed data saved to:\", output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import signal, stats\n",
        "from scipy.fft import fft\n",
        "\n",
        "# Section 1: Load Data\n",
        "file_path = 'Forward_1.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Clean column names (remove leading/trailing whitespace)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Ensure numeric data types for relevant columns\n",
        "for col in ['ax', 'ay', 'az', 'wx', 'wy', 'wz', 'Bx', 'By', 'Bz']:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Calculate total acceleration\n",
        "df['total_accel'] = np.sqrt(df['ax']**2 + df['ay']**2 + df['az']**2)\n",
        "\n",
        "# Detect peaks in the total acceleration\n",
        "peaks, _ = signal.find_peaks(df['total_accel'], height=1.0)\n",
        "\n",
        "# Feature extraction function (26 features)\n",
        "def extract_features(segment, axis):\n",
        "    \"\"\"Extracts 26 features from a single axis segment of accelerometer, gyroscope, or magnetometer data.\"\"\"\n",
        "\n",
        "    # Basic Statistical Features\n",
        "    min_val = segment[axis].min()\n",
        "    max_val = segment[axis].max()\n",
        "    mean_val = segment[axis].mean()\n",
        "    skewness = stats.skew(segment[axis].dropna())\n",
        "    kurtosis = stats.kurtosis(segment[axis].dropna())\n",
        "\n",
        "    # Autocorrelation Features (lags 1-11)\n",
        "    autocorr_vals = [segment[axis].autocorr(lag=i) for i in range(1, 12)]\n",
        "\n",
        "    # Frequency Domain Features (First 5 frequencies and their amplitudes)\n",
        "    segment_data = segment[axis].fillna(0).to_numpy()\n",
        "    freq_data = np.abs(fft(segment_data))  # Magnitude of DFT\n",
        "    sorted_indices = np.argsort(freq_data)[::-1]  # Sort frequencies by magnitude, descending\n",
        "    top_freqs = sorted_indices[:5]\n",
        "    top_amplitudes = freq_data[top_freqs]\n",
        "\n",
        "    features = {\n",
        "        f'{axis}_min': min_val,\n",
        "        f'{axis}_max': max_val,\n",
        "        f'{axis}_mean': mean_val,\n",
        "        f'{axis}_skewness': skewness,\n",
        "        f'{axis}_kurtosis': kurtosis,\n",
        "    }\n",
        "\n",
        "    # Add autocorrelation features\n",
        "    for i, autocorr_val in enumerate(autocorr_vals, start=1):\n",
        "        features[f'{axis}_autocorr_lag_{i}'] = autocorr_val\n",
        "\n",
        "    # Add frequency and amplitude features\n",
        "    for i, (freq, amp) in enumerate(zip(top_freqs, top_amplitudes), start=1):\n",
        "        features[f'{axis}_freq_{i}'] = freq\n",
        "        features[f'{axis}_amplitude_{i}'] = amp\n",
        "\n",
        "    return pd.Series(features)\n",
        "\n",
        "# Section 6: Apply Feature Extraction around Peaks\n",
        "all_features = []\n",
        "for peak in peaks:\n",
        "    if peak > 50 and peak < len(df) - 50:  # Ensure there's data around the peak\n",
        "        segment = df.iloc[peak-50:peak+50]  # Get 50 samples before and after\n",
        "\n",
        "        # Extract features for each axis of accelerometer, gyroscope, and magnetometer data\n",
        "        features = pd.concat([\n",
        "            extract_features(segment, 'ax'),\n",
        "            extract_features(segment, 'ay'),\n",
        "            extract_features(segment, 'az'),\n",
        "            extract_features(segment, 'wx'),\n",
        "            extract_features(segment, 'wy'),\n",
        "            extract_features(segment, 'wz'),\n",
        "            extract_features(segment, 'Bx'),\n",
        "            extract_features(segment, 'By'),\n",
        "            extract_features(segment, 'Bz')\n",
        "        ])\n",
        "\n",
        "        # Label features based on peak event (manually set here for demonstration)\n",
        "        # In a real scenario, you would programmatically determine these based on fall data or user input\n",
        "        features['fall'] = 1  # 1 for fall, 0 for non-fall; adjust as needed\n",
        "        features['fall_direction'] = 'forward'  # Options: 'forward', 'backward', 'lateral'\n",
        "\n",
        "        all_features.append(features)\n",
        "\n",
        "# Create a DataFrame with all extracted features\n",
        "features_df = pd.DataFrame(all_features)\n",
        "\n",
        "# Section 7: Save Preprocessed Data\n",
        "output_file_path = 'processed_fall_data_features.xlsx'\n",
        "features_df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(\"Preprocessed data with features saved to:\", output_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os5W0sNQC1MR",
        "outputId": "f463428b-3a13-4223-e2d1-c4dc25d1b658"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed data with features saved to: processed_fall_data_features.xlsx\n"
          ]
        }
      ]
    }
  ]
}